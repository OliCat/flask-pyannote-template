# Guide: Pyannote MPS avec Flask/Gunicorn

## üéØ Probl√©matique identifi√©e

**Flask/Gunicorn peut causer des crashs m√©moire avec MPS**, m√™me si MPS fonctionne bien dans une application standalone.

## üîç Pourquoi Flask/Gunicorn pose probl√®me avec MPS ?

### Probl√®mes sp√©cifiques

1. **Workers multiples**
   - Gunicorn lance plusieurs workers (processus ou threads)
   - Chaque worker peut essayer d'utiliser MPS
   - **Probl√®me**: MPS ne partage pas bien les ressources entre processus
   - **R√©sultat**: Crashs OOM (Out Of Memory)

2. **Gestion m√©moire des workers**
   - Les workers r√©utilis√©s peuvent garder la m√©moire GPU
   - Pas de nettoyage automatique entre les requ√™tes
   - **Probl√®me**: Accumulation de m√©moire MPS
   - **R√©sultat**: Crashs progressifs

3. **Partage de contexte MPS**
   - MPS n'est pas thread-safe pour le partage de contexte
   - Workers threads partagent le m√™me espace m√©moire
   - **Probl√®me**: Conflits d'acc√®s GPU
   - **R√©sultat**: Crashs al√©atoires

4. **Charge concurrente**
   - Plusieurs requ√™tes simultan√©es = plusieurs workers actifs
   - Chaque worker charge son propre mod√®le pyannote
   - **Probl√®me**: Multiplication de l'utilisation m√©moire
   - **R√©sultat**: Crashs sous charge

---

## ‚úÖ Solutions pour Flask/Gunicorn

### Solution 1: Workers = 1 (Simple mais limit√©)

**Configuration Gunicorn:**
```bash
gunicorn --workers 1 --threads 4 app:app
```

**Avantages:**
- ‚úÖ √âvite les conflits MPS entre workers
- ‚úÖ Simple √† impl√©menter

**Inconv√©nients:**
- ‚ö†Ô∏è Pas de parall√©lisation vraie
- ‚ö†Ô∏è Limite la capacit√© concurrente

**Quand utiliser:**
- Tests et d√©veloppement
- Charge faible
- Environnement de production limit√©

---

### Solution 2: CPU par d√©faut, MPS optionnel (Recommand√©)

**Strat√©gie:**
- **Par d√©faut**: CPU (stable pour tous les workers)
- **MPS optionnel**: Uniquement si worker=1 et requ√™te explicite

**Impl√©mentation:**
```python
from pyannote_mps_helper import create_pyannote_pipeline_safe
import os

def get_pipeline(use_mps=False):
    """Cr√©e un pipeline pyannote de mani√®re s√©curis√©e"""
    
    # D√©tection automatique: MPS seulement si worker unique
    gunicorn_workers = os.environ.get('GUNICORN_WORKERS', '1')
    
    if use_mps and int(gunicorn_workers) > 1:
        print("‚ö†Ô∏è MPS d√©sactiv√©: Gunicorn a plusieurs workers")
        use_mps = False
    
    pipeline = create_pyannote_pipeline_safe(
        prefer_mps=use_mps,
        embedding_batch_size=16
    )
    
    return pipeline
```

**Configuration Gunicorn:**
```bash
# Pour CPU (recommand√©)
gunicorn --workers 4 --threads 2 app:app

# Pour MPS (si vraiment n√©cessaire)
GUNICORN_WORKERS=1 gunicorn --workers 1 --threads 4 app:app
```

---

### Solution 3: Pool de pipelines pr√©charg√©s (Avanc√©)

**Strat√©gie:**
- Pr√©charger un pool de pipelines au d√©marrage
- Workers r√©utilisent les pipelines du pool
- Gestion m√©moire centralis√©e

**Impl√©mentation:**
```python
from queue import Queue
import threading

class PipelinePool:
    def __init__(self, pool_size=2, use_mps=False):
        self.pool = Queue(maxsize=pool_size)
        self.use_mps = use_mps and pool_size == 1  # MPS seulement si pool=1
        
        # Pr√©charger les pipelines
        for _ in range(pool_size):
            pipeline = create_pyannote_pipeline_safe(
                prefer_mps=self.use_mps,
                embedding_batch_size=16
            )
            self.pool.put(pipeline)
    
    def get(self):
        return self.pool.get()
    
    def put(self, pipeline):
        # Nettoyer avant de remettre
        if self.use_mps:
            torch.mps.empty_cache()
        gc.collect()
        self.pool.put(pipeline)

# Pool global (initialis√© au d√©marrage)
pipeline_pool = PipelinePool(pool_size=2, use_mps=False)

@app.route('/transcribe', methods=['POST'])
def transcribe():
    pipeline = pipeline_pool.get()
    try:
        result = pipeline(audio_file)
        return result
    finally:
        pipeline_pool.put(pipeline)
```

**Configuration:**
- **CPU**: `pool_size=2-4` (selon RAM)
- **MPS**: `pool_size=1` (un seul pipeline MPS)

---

### Solution 4: Worker sp√©cialis√© MPS (Architecture recommand√©e)

**Strat√©gie:**
- **Worker 1**: MPS (un seul worker d√©di√©)
- **Workers 2-N**: CPU (workers normaux)
- Route les requ√™tes selon disponibilit√©

**Architecture:**
```
Gunicorn
‚îú‚îÄ‚îÄ Worker 1 (MPS) - Traite les requ√™tes prioritaires
‚îú‚îÄ‚îÄ Worker 2 (CPU) - Traite les requ√™tes normales
‚îú‚îÄ‚îÄ Worker 3 (CPU) - Traite les requ√™tes normales
‚îî‚îÄ‚îÄ Worker 4 (CPU) - Traite les requ√™tes normales
```

**Impl√©mentation:**
```python
import os

def create_pipeline_for_worker():
    """Cr√©e un pipeline selon le num√©ro de worker"""
    worker_id = os.environ.get('GUNICORN_WORKER_ID', '0')
    
    # Worker 1 utilise MPS, autres utilisent CPU
    use_mps = (worker_id == '1')
    
    return create_pyannote_pipeline_safe(
        prefer_mps=use_mps,
        embedding_batch_size=16 if use_mps else 32
    )

# Pipeline charg√© une fois par worker
pipeline = create_pipeline_for_worker()
```

**Configuration Gunicorn:**
```bash
gunicorn --workers 4 --threads 2 \
  --worker-class sync \
  --env GUNICORN_WORKER_ID={{ worker_id }} \
  app:app
```

---

## üõ†Ô∏è Configuration recommand√©e pour votre projet Flask

### Pour la stabilit√© (Production)

```python
# config.py
import os

# D√©tection automatique de l'environnement
IS_GUNICORN = 'gunicorn' in os.environ.get('SERVER_SOFTWARE', '')
WORKER_COUNT = int(os.environ.get('GUNICORN_WORKERS', '1'))

# Configuration MPS
USE_MPS = False  # Par d√©faut d√©sactiv√© pour Flask/Gunicorn
if IS_GUNICORN:
    # MPS seulement si un seul worker
    USE_MPS = USE_MPS and WORKER_COUNT == 1
    EMBEDDING_BATCH_SIZE = 16  # Conservateur pour √©viter OOM
else:
    # Application standalone peut utiliser MPS
    USE_MPS = True
    EMBEDDING_BATCH_SIZE = 16

# Fonction helper
def get_pyannote_pipeline():
    from pyannote_mps_helper import create_pyannote_pipeline_safe
    
    return create_pyannote_pipeline_safe(
        prefer_mps=USE_MPS,
        embedding_batch_size=EMBEDDING_BATCH_SIZE
    )
```

### Dans votre route Flask

```python
from flask import Flask, request, jsonify
from config import get_pyannote_pipeline

app = Flask(__name__)

# Pipeline charg√© une fois au d√©marrage du worker
pipeline = get_pyannote_pipeline()

@app.route('/api/diarize', methods=['POST'])
def diarize():
    audio_file = request.files['audio']
    
    try:
        # Traitement avec gestion m√©moire
        from pyannote_mps_helper import process_with_memory_management
        import torch
        
        device = torch.device('mps' if 'mps' in str(pipeline.device) else 'cpu')
        result = process_with_memory_management(pipeline, audio_file, device)
        
        return jsonify({
            'success': True,
            'result': result
        })
    except Exception as e:
        # Fallback vers CPU si erreur
        pipeline.to(torch.device('cpu'))
        result = pipeline(audio_file)
        
        return jsonify({
            'success': True,
            'result': result,
            'fallback_cpu': True
        })
```

---

## üîß Configuration Gunicorn optimale

### Pour CPU (Recommand√© pour production)

```bash
# gunicorn_config.py
workers = 4
threads = 2
worker_class = 'sync'
worker_connections = 1000
timeout = 300
keepalive = 5
```

```bash
gunicorn -c gunicorn_config.py app:app
```

### Pour MPS (Test seulement)

```bash
# MPS n√©cessite un seul worker
gunicorn --workers 1 --threads 4 --timeout 600 app:app
```

**‚ö†Ô∏è Limitation**: Un seul worker = pas de parall√©lisation vraie

---

## üìä Comparaison des approches

| Approche | Workers | MPS | Stabilit√© | Performance | Production |
|----------|---------|-----|-----------|-------------|------------|
| **1 Worker** | 1 | ‚úÖ | ‚ö†Ô∏è | ‚ö° | ‚ùå |
| **CPU par d√©faut** | 4+ | ‚ùå | ‚úÖ | üê¢ | ‚úÖ |
| **Pool pipelines** | 2-4 | ‚ö†Ô∏è | ‚úÖ | ‚ö° | ‚úÖ |
| **Worker MPS d√©di√©** | 4 (1 MPS) | ‚úÖ | ‚úÖ | ‚ö°‚ö° | ‚úÖ |

---

## üí° Recommandation finale

### Pour votre projet Flask/Gunicorn :

1. **Production**: CPU par d√©faut, workers multiples
   ```python
   USE_MPS = False  # Stable pour tous les workers
   WORKERS = 4      # Bonne parall√©lisation
   ```

2. **D√©veloppement/Test**: MPS optionnel avec worker unique
   ```python
   USE_MPS = True   # Test seulement
   WORKERS = 1      # √âvite les conflits
   ```

3. **Hybride**: Worker d√©di√© MPS + workers CPU
   - Meilleur compromis performance/stabilit√©
   - Plus complexe √† mettre en place

---

## üö® Points d'attention

1. **Nettoyage m√©moire obligatoire**
   - Appeler `torch.mps.empty_cache()` entre requ√™tes
   - Utiliser `process_with_memory_management()` du helper

2. **Pas de partage de pipeline entre workers**
   - Chaque worker doit avoir son propre pipeline
   - Pas de variable globale partag√©e

3. **Gestion des erreurs OOM**
   - Toujours pr√©voir un fallback CPU
   - Logger les erreurs pour debugging

4. **Monitoring**
   - Surveiller la m√©moire GPU dans Activity Monitor
   - Logger les temps de traitement
   - Alerter si trop d'OOM

---

## ‚úÖ Checklist d'impl√©mentation

- [ ] Utiliser `pyannote_mps_helper.py` (d√©j√† cr√©√©)
- [ ] D√©sactiver MPS par d√©faut si `workers > 1`
- [ ] Impl√©menter fallback CPU automatique
- [ ] Nettoyer la m√©moire entre requ√™tes
- [ ] Tester avec un worker (MPS)
- [ ] Tester avec plusieurs workers (CPU)
- [ ] Monitorer la m√©moire en production
- [ ] Documenter la configuration

---

**Conclusion**: Flask/Gunicorn avec plusieurs workers ne fonctionne pas bien avec MPS. Utilisez CPU par d√©faut en production, MPS uniquement pour tests avec un seul worker.

