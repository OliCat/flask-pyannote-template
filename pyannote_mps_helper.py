#!/usr/bin/env python3
"""
Helper pour utiliser MPS de mani√®re stable avec pyannote.audio
Bas√© sur les meilleures pratiques pour √©viter les crashs de m√©moire

R√©f√©rences:
- https://apxml.com/posts/pytorch-macos-metal-gpu
- Probl√®mes de m√©moire MPS document√©s dans la communaut√© PyTorch
"""

import torch
from pyannote.audio import Pipeline
import gc
import warnings


def get_safe_device(prefer_mps=False, fallback_to_cpu=True):
    """
    D√©termine le device le plus s√ªr √† utiliser avec pyannote.
    
    Args:
        prefer_mps: Si True, essaie d'utiliser MPS si disponible
        fallback_to_cpu: Si True, retourne CPU si MPS pose probl√®me
    
    Returns:
        torch.device: Device √† utiliser
    """
    if prefer_mps and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        # V√©rifier que MPS est vraiment fonctionnel
        try:
            # Test rapide: cr√©er un petit tenseur sur MPS
            test_tensor = torch.randn(10, 10).to('mps')
            _ = test_tensor * 2  # Op√©ration simple
            del test_tensor
            torch.mps.empty_cache()  # Nettoyer
            gc.collect()
            return torch.device('mps')
        except Exception as e:
            warnings.warn(f"MPS disponible mais test √©chou√©: {e}. Fallback vers CPU.")
            if fallback_to_cpu:
                return torch.device('cpu')
    
    # CPU par d√©faut (le plus stable)
    return torch.device('cpu')


def create_pyannote_pipeline_safe(
    model_name="pyannote/speaker-diarization-3.1",
    use_auth_token=True,
    prefer_mps=False,
    embedding_batch_size=None
):
    """
    Cr√©e un pipeline pyannote de mani√®re s√ªre avec gestion du device.
    
    Args:
        model_name: Nom du mod√®le pyannote
        use_auth_token: Token HuggingFace
        prefer_mps: Si True, essaie d'utiliser MPS
        embedding_batch_size: Taille de batch pour l'embedding (plus petit = moins de m√©moire)
    
    Returns:
        Pipeline: Pipeline pyannote configur√©
    """
    device = get_safe_device(prefer_mps=prefer_mps)
    
    print(f"üîß Cr√©ation du pipeline pyannote sur device: {device}")
    
    # Cr√©er le pipeline
    pipeline = Pipeline.from_pretrained(
        model_name,
        use_auth_token=use_auth_token
    )
    
    # Configurations pour MPS si n√©cessaire
    if device.type == 'mps':
        print("üçé Configuration MPS activ√©e")
        print("   ‚ö†Ô∏è Mode exp√©rimental - en cas de crash, utiliser CPU")
        
        # R√©duire la taille de batch par d√©faut pour √©conomiser la m√©moire
        # Les tailles de 16-64 sont recommand√©es pour MPS
        if embedding_batch_size is None:
            embedding_batch_size = 16  # Plus petit que la valeur par d√©faut
            print(f"   üì¶ Taille de batch r√©duite √† {embedding_batch_size} pour MPS")
        
        if hasattr(pipeline, 'embedding_batch_size'):
            pipeline.embedding_batch_size = embedding_batch_size
    
    # D√©placer le pipeline sur le device choisi
    try:
        pipeline.to(device)
        print(f"‚úÖ Pipeline d√©plac√© vers {device}")
        
        # V√©rification: s'assurer que les mod√®les sont bien sur le bon device
        if hasattr(pipeline, '_segmentation') and hasattr(pipeline._segmentation, 'model'):
            seg_model = pipeline._segmentation.model
            if hasattr(seg_model, 'parameters'):
                first_param = next(iter(seg_model.parameters()))
                actual_device = str(first_param.device)
                if actual_device != str(device):
                    print(f"   ‚ö†Ô∏è Avertissement: device attendu {device}, d√©tect√© {actual_device}")
                else:
                    print(f"   ‚úì V√©rification device: {actual_device}")
        
        return pipeline
    
    except Exception as e:
        print(f"‚ùå Erreur lors du d√©placement vers {device}: {e}")
        if device.type == 'mps':
            print("   üîÑ Tentative de fallback vers CPU...")
            device = torch.device('cpu')
            pipeline.to(device)
            print(f"   ‚úÖ Fallback r√©ussi: utilisation de CPU")
        return pipeline


def process_with_memory_management(pipeline, audio_file, device):
    """
    Traite un fichier audio avec gestion proactive de la m√©moire.
    Utile pour √©viter les crashs MPS dus aux probl√®mes de m√©moire.
    """
    # Nettoyer avant le traitement
    if device.type == 'mps':
        torch.mps.empty_cache()
    elif device.type == 'cuda':
        torch.cuda.empty_cache()
    gc.collect()
    
    try:
        # Traitement
        result = pipeline(audio_file)
        
        # Nettoyer apr√®s le traitement
        if device.type == 'mps':
            torch.mps.empty_cache()
        elif device.type == 'cuda':
            torch.cuda.empty_cache()
        gc.collect()
        
        return result
    
    except RuntimeError as e:
        if 'out of memory' in str(e).lower() or 'memory' in str(e).lower():
            print(f"‚ö†Ô∏è Erreur de m√©moire d√©tect√©e: {e}")
            if device.type == 'mps':
                print("   üí° Suggestions:")
                print("      - R√©duire embedding_batch_size")
                print("      - Utiliser CPU √† la place (plus stable)")
                print("      - Traiter des fichiers audio plus courts")
        raise


# Exemple d'utilisation
if __name__ == "__main__":
    print("=" * 60)
    print("üîß Helper pour pyannote avec MPS s√©curis√©")
    print("=" * 60)
    
    # Test 1: CPU (le plus s√ªr)
    print("\n1Ô∏è‚É£ Test avec CPU (mode recommand√©):")
    try:
        pipeline_cpu = create_pyannote_pipeline_safe(prefer_mps=False)
        print("   ‚úÖ Pipeline CPU cr√©√© avec succ√®s")
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
    
    # Test 2: MPS si disponible (exp√©rimental)
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("\n2Ô∏è‚É£ Test avec MPS (mode exp√©rimental):")
        try:
            pipeline_mps = create_pyannote_pipeline_safe(
                prefer_mps=True,
                embedding_batch_size=16  # Taille r√©duite pour MPS
            )
            print("   ‚úÖ Pipeline MPS cr√©√© avec succ√®s")
            print("   ‚ö†Ô∏è Note: En cas de crash, utiliser CPU")
        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            print("   üí° Recommandation: Utiliser CPU (mode pr√©f√©r√©)")
    else:
        print("\n2Ô∏è‚É£ MPS non disponible sur ce syst√®me")
    
    print("\n" + "=" * 60)
    print("üí° RECOMMANDATIONS:")
    print("   - CPU: Le plus stable, recommand√© pour la production")
    print("   - MPS: Plus rapide mais peut crasher (mode exp√©rimental)")
    print("   - En cas de crash MPS: r√©duire embedding_batch_size ou utiliser CPU")
    print("=" * 60)

